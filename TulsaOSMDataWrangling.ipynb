{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenStreetMap Data Wrangling with MongoDB\n",
    "## Jenni Sanders\n",
    "\n",
    "#### Map Area: Tulsa, OK, United States"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems Encountered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Abbreviated and misspelled street names\n",
    "  * Ave. instead of Avenue\n",
    "  * Bouleavard instead of Boulevard\n",
    "* Second-level Tags\n",
    "  * payment:electronic_purses\n",
    "  * addr:street\n",
    "* Phone Number Formatting\n",
    "\n",
    "Additionally, the most common top-level tag found in the data was 'tiger'.  I considered this unusual, did some research, and these tags are part of the TIGER import of the United States (http://wiki.openstreetmap.org/wiki/Key:tiger)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abbreviated and Misspelled Street Names\n",
    "\n",
    "There were a lot of street names where the type of street, e.g. Avenue or Boulevard, was either abbreviated or mispelled.  I've corrected those where possible.  When a street type is missing, e.g. 'South Memorial' instead of 'South Memorial Drive', I haven't assumed what street type should be present.\n",
    "\n",
    "#### Second-level Tags\n",
    "Out of the 421160 tags in the data, 196217 or just under half had second-level tags.  Because they are in the original data like tag1:tag2, I decided to make these so that the second-level tags, e.g. electronic_purses, would be a subset of payment.\n",
    "\n",
    "#### Phone Number Formatting\n",
    "Phone numbers were formatted in multiple ways, including +1 prefixes before the number, parenthesis inside the number, and occasionally periods within the number.\n",
    "```python\n",
    "numeric = re.compile(r'^([0-9])*$')\n",
    "numericdashes = re.compile(r'^([0-9]|\\-)*$')\n",
    "\n",
    "def phone_number_format(element, keys):\n",
    "    other_formatted_numbers = []\n",
    "    problem_formatted_numbers = []\n",
    "    \n",
    "    if element.tag == \"tag\" and element.attrib['k'] == \"phone\":\n",
    "        if numeric.match(element.attrib['v']):\n",
    "            keys['numeric'] += 1\n",
    "        if numericdashes.match(element.attrib['v']):\n",
    "            keys['numericdashes'] += 1\n",
    "        elif element.attrib['v'][0] == '+' and numericdashes.match(element.attrib['v'][1:]):\n",
    "            keys['beginwplus'] += 1\n",
    "        elif problemchars.search(element.attrib['v']) is not None:\n",
    "            keys['problemchars'] += 1\n",
    "            problem_formatted_numbers.append(element.attrib['v'])\n",
    "        else:\n",
    "            keys['other'] += 1\n",
    "            other_formatted_numbers.append(element.attrib['v'])\n",
    "    \n",
    "#     if len(other_formatted_numbers) > 0:\n",
    "#         print other_formatted_numbers\n",
    "    \n",
    "#     if len(problem_formatted_numbers) > 0:\n",
    "#         print problem_formatted_numbers\n",
    "    \n",
    "    return keys\n",
    "\n",
    "\n",
    "def process_map_phonekeys(filename):\n",
    "    keys = {\"numeric\": 0, \"numericdashes\": 0, \"beginwplus\":0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = phone_number_format(element, keys)\n",
    "\n",
    "    return keys\n",
    "\n",
    "phonekeys = process_map_phonekeys(OSMFILE)\n",
    "pprint.pprint(phonekeys)```\n",
    "\n",
    "I used the phonenumbers library to normalize these to the US national number format.\n",
    "```python\n",
    "import phonenumbers as pn\n",
    "\n",
    "def tidy_phone(phonenumber):\n",
    "    return pn.format_number(pn.parse(phonenumber, 'US'), pn.PhoneNumberFormat.NATIONAL)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extract was a custom extract from Mapzen with a key of odes-agCKTKY.  This extract consists of 86,113 KB.  When exported to JSON, this becomes a 133,866 KB file.  The storage size in Mongo is 33734656, with 396501 objects.  \n",
    "\n",
    "These objects are made of 339454 nodes:  \n",
    "```python\n",
    "db.OSM.find( { \"type\" : \"node\" }).count()```  \n",
    "\n",
    "and 57047 ways:  \n",
    "```python\n",
    "db.OSM.find( { \"type\" : \"way\" }).count()```\n",
    "\n",
    "contributed by 371 users.\n",
    "```python\n",
    "def count_users_pipeline():\n",
    "    pipeline = [\n",
    "        { \"$match\" : { \"user\" : { \"$exists\" : True }}}, ##only count changes with a username attached\n",
    "        { \"$group\" : { \"_id\" : \"$user\", \"count\": { \"$sum\" : 1 }}},\n",
    "        { \"$group\" : { \"_id\" : \"unique users\", \"count\" : { \"$sum\" : 1 }}}\n",
    "    ]\n",
    "    return pipeline\n",
    "    \n",
    "pipeline = count_users_pipeline()\n",
    "result = aggregate(db, pipeline)\n",
    "pprint.pprint(result[0])```\n",
    "\n",
    "The most active user in the dataset is Paul Johnson, with 133,819 changes.\n",
    "```python\n",
    "def user_pipeline():\n",
    "    pipeline = [\n",
    "        { \"$match\" : { \"user\" : { \"$exists\" : True }}}, ##only count changes with a username attached\n",
    "        { \"$group\" : { \"_id\" : \"$user\", \"count\": { \"$sum\" : 1 }}},\n",
    "        { \"$sort\" : { \"count\" : -1 }},\n",
    "        { \"$limit\" : 1 }\n",
    "    ]\n",
    "    return pipeline\n",
    "\n",
    "def aggregate(db, pipeline):\n",
    "    return [doc for doc in db.OSM.aggregate(pipeline)]\n",
    "\n",
    "pipeline = user_pipeline()\n",
    "result = aggregate(db, pipeline)\n",
    "pprint.pprint(result[0])```\n",
    "\n",
    "This name sounded familiar, so I did some searching and in 2014 this person had over a million nodes in total changes. https://codefortulsa.org/2014/01/19/tulsas-first-openstreetmap-editathon/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be interesting to reconcile this data set with the USPS data set to determine which addresses in the set are valid, and potentially reconcile incomplete or incorrect addresses to improve data accuracy and reliability.  For example, the following address is in the data set: \n",
    "\n",
    "```python\n",
    "for node in db.OSM.find({'address.street' : {'$regex' : 'Tulsa Promenade'}}):\n",
    "    pprint.pprint(node)```\n",
    "    \n",
    "148 Tulsa Promenade, should be:\n",
    "\n",
    "   TULSA PROMENADE  \n",
    "   4107 S YALE AVE  \n",
    "   STE 148  \n",
    "   TULSA OK 74135-6015  \n",
    "   \n",
    "Data could be fed through a package like pyusps or postal-address to improve accuracy and normalize addresses.  However, given the manual nature of many of the Open Street Map entries, human verification would be recommended for at least a sample subset of this data to validate accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Exploration  \n",
    "There's a running joke in Tulsa that there are an equal number of bars and churches.  Analyzing the data set, bars aren't even in the top five amenities:\n",
    "```python\n",
    "def count_amenities_pipeline():\n",
    "    pipeline = [\n",
    "        { \"$match\" : {\"amenity\" : { \"$exists\" : True }, \"type\" : \"node\" }}, #only nodes with an amenity tag\n",
    "        { \"$group\" : { \"_id\" : \"$amenity\", \"count\": {\"$sum\" : 1 }}},\n",
    "        { \"$sort\" : { \"count\" : -1 }}\n",
    "    ]\n",
    "    return pipeline\n",
    "\n",
    "pipeline = count_amenities_pipeline()\n",
    "result = aggregate(db, pipeline)\n",
    "pprint.pprint(result[0:5])```\n",
    "\n",
    "The top five are:\n",
    "* Place of Worship, 554\n",
    "* School, 140\n",
    "* Fast Food, 92\n",
    "* Restaurant, 76\n",
    "* Fountain, 67"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even assuming some self-selection, that bar number looks pretty low.  I looked through all the names of amenities to determine what types of ameneties appeared to be places where the primary source of income would be alcohol sales.  Using that list, I looked at just those establishments and places of worship to determine the numbers between the two:\n",
    "``` python\n",
    "def count_certain_amenities_pipeline():\n",
    "    pipeline = [\n",
    "        { \"$match\" : {\"amenity\" : { \"$in\" : [\"place_of_worship\", \"bar\", \"pub\", \"liquor\", \"nightclub\"]} , \"type\" : \"node\" }}, #only the nodes we're interested in\n",
    "        { \"$group\" : { \"_id\" : \"$amenity\", \"count\": {\"$sum\" : 1 }}},\n",
    "        { \"$sort\" : { \"count\" : -1 }}\n",
    "    ]\n",
    "    return pipeline\n",
    "\n",
    "pipeline = count_certain_amenities_pipeline()\n",
    "result = aggregate(db, pipeline)\n",
    "pprint.pprint(result)```\n",
    "\n",
    "* Place of Worship, 554\n",
    "* Bar, 9\n",
    "* Pub, 8\n",
    "* Liquor, 2\n",
    "* Nightclub, 1\n",
    "\n",
    "Given the above totals, it would be far easier to use this data set to find a place of worship than a watering hole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The Tulsa data set seems fairly clean, especially considering there were _no_ problematic characters in the tags.  It could use some address scrubbing, as is expected with a manually created data set.  Whenever people touch data, it's likely they will use shortcuts, skip unknown data, and make typos in the data.  For example, I've frequently found 111111111 or 999999999 listed as social security numbers in other data sets.  A bot to rectify address data in this set with a known accurate data source would be a large step forward in data accuracy in this set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "7.2. re — Regular expression operations¶. (n.d.). Retrieved January 02, 2017, from https://docs.python.org/2/library/re.html#simulating-scanf  \n",
    "Delete everything in a MongoDB database. (n.d.). Retrieved January 02, 2017, from http://stackoverflow.com/questions/3366397/delete-everything-in-a-mongodb-database  \n",
    "Efficient way to remove keys with empty values from a dict. (n.d.). Retrieved January 02, 2017, from http://stackoverflow.com/questions/12118695/efficient-way-to-remove-keys-with-empty-values-from-a-dict  \n",
    "Find or Query Data with the mongo Shell¶. (n.d.). Retrieved January 02, 2017, from https://docs.mongodb.com/getting-started/shell/query/  \n",
    "How do I query mongodb with \"like\"? (n.d.). Retrieved January 02, 2017, from http://stackoverflow.com/questions/3305561/how-do-i-query-mongodb-with-like  \n",
    "Install MongoDB Community Edition on Windows¶. (n.d.). Retrieved January 02, 2017, from https://docs.mongodb.com/manual/tutorial/install-mongodb-on-windows/  \n",
    "MongoDB select count(distinct x) on an indexed column - count unique results for large data sets. (n.d.). Retrieved January 02, 2017, from http://stackoverflow.com/questions/11782566/mongodb-select-countdistinct-x-on-an-indexed-column-count-unique-results-for  \n",
    "Pymongo method of getting statistics for collection byte usage? (n.d.). Retrieved January 02, 2017, from http://stackoverflow.com/questions/18836064/pymongo-method-of-getting-statistics-for-collection-byte-usage  \n",
    "Python Regex Cheatsheet. (n.d.). Retrieved January 2, 2017, from https://www.debuggex.com/cheatsheet/regex/python  \n",
    "B. (2014, January 19). Tulsa’s First OpenStreetMap Editathon. Retrieved January 02, 2017, from https://codefortulsa.org/2014/01/19/tulsas-first-openstreetmap-editathon/  \n",
    "Udacity | Free Online Courses & Nanodegree Programs - Udacity. (n.d.). Retrieved January 02, 2017, from https://discussions.udacity.com/t/how-to-set-up-mongodb-locally-windows/185014/2"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
